# 第七讲 2D-2D

标签（空格分隔）： SLAM

---
**对极几何**

- 对极约束
1. 用归一化平面坐标$x_1$、$x_2$
$x_2^Tt$^$Rx_1=0$
**（自己理解：分为两部分,理解成“：$x_2$平移t后与$x_1$旋转R后重合）**
**（理解矩阵后改为：$x_1$先旋转R再平移t后与$x_2$重合）**

2. 用像素坐标$p_1$、$p_2$
$p_1$、$p_2$为：$p_2^TK^{-T}t$^$RK^{-1}p_1=0$

（x和p之间相差一内参K，即$p_1=Kx_1$或$x_1=K^{-1}p_1$）

---
**本质矩阵E**
将归一化相机坐标的对极约束中间部分定义为E，即$E=t$^$R$
**基本矩阵F**
将像素坐标的对极约束中间部分定义为F，即$F=K^{-T}EK^{-1}=K^{-T}t$^$RK^{-1}$

所以对极约束简化为：
$x_2^TEx_1=0$（归一化相机坐标）
$p_2^TFp_1=0$（像素坐标）

---
**本质矩阵求解相机位姿：**
```flow
st=>start: 两帧图像特征匹配
op1=>operation: 八点法（归一化相机坐标）求解E
op2=>operation: 对E进行SVD（奇异值）分解得四个解
end=>end: 选取x1和x2的深度都为正的那个解作为相机位姿R,t

st->op1->op2->end
```


- 由于对极约束是等于0的式子（乘上一个数不影响），所以可以说**E具有尺度等价性**
- 对E进行SVD分解：$E=U\sum V^T$
$\sum$为奇异值矩阵，根据E的内在性质应有$\sum=diag(\sigma,\sigma,0)$,但线性方程（八点法）解得的E不一定符合。
若不满足则可以直接取成$\sum=diag(1,1,0)$（基于E的尺度等价性）

---
**基本矩阵求解相机位姿**
步骤与上面一样，只不过是用像素坐标来解得F

---
**单应矩阵** 
直接表示特征点在两帧图像中所得的两个**像素坐标**的关系(跟F一样)

- 适用于两帧图像匹配的特征点恰好都在一个平面内的情况

- 空间平面表示：
$Ax+By+Cz+D=0$(A,B,C不同时为0）
表示成“点的集合”形式：
$n^TP+d=0$ （$n=[A,B,C]^T$为平面法向量，P为平面上的点）

 $p_2=K(R-{{tn^T}\over d})K^{-1}p_1$
将中间部分定义为单应矩阵H，得：
$p_2=Hp_1$

**F、H描述像素坐标的变换，E描述归一化相机坐标的变换**

**单应矩阵求解相机位姿**
```flow
st=>start: 两帧图像特征匹配
op1=>operation: 四点法（像素坐标）求解H
op2=>operation: 对H进行分解
end=>end: 解得相机位姿R,t

st->op1->op2->end
```

---
**三角测量**

- 对极几何得到相机位姿
- 三角测量得到点的深度（即相机坐标的z值）
- 用OpenCV里的函数运算

   $s_1x_1=Rs_2x_2+t$
$s_1x_1$^$x_1=0=s_2x_1$^$Rx_2+x_1$^$t$
第二个式子右边等式求出$s_2$，再用第一个式子求出$s_1$
**由于噪声存在，一般用最小二乘代替准确地解方程**

- 三角化的矛盾
 - 必须有平移才能有对极几何，才能三角化
 - 平移太小，三角化精度不够
 - 平移太大，可能导致匹配失败
![三角化矛盾][1]



 ---
**2D-2D（对极几何）求解相机位姿和特征点的世界坐标**

```flow
st=>start: 两帧图像特征匹配得到两个像素坐标（或乘K-1变成归一化相机坐标）
op1=>operation: 求出R、t( 对极几何、八点法、分解E/四点法、分解H )
op2=>operation: 求出特征点深度z（三角测量）
end=>end: 得到相机坐标[x,y,z]

st->op1->op2->end
```



---

- 三角化点与特征点的重投影验证
把三角化解出的点再投影到成像平面上（即用估计出的世界坐标恢复的归一化相机坐标，与从像素坐标恢复的归一化相机坐标比较，得到的误差为**重投影误差**）
```
for ( int i=0; i<matches.size(); i++ )
    {
        Point2d pt1_cam = pixel2cam( keypoints_1[ matches[i].queryIdx ].pt, K );
        Point2d pt1_cam_3d(
            points[i].x/points[i].z, 
            points[i].y/points[i].z 
        );
        
        cout<<"point in the first camera frame: "<<pt1_cam<<endl;
        cout<<"point projected from 3D "<<pt1_cam_3d<<", d="<<points[i].z<<endl;
        
        // 第二个图
        Point2f pt2_cam = pixel2cam( keypoints_2[ matches[i].trainIdx ].pt, K );
        Mat pt2_trans = R*( Mat_<double>(3,1) << points[i].x, points[i].y, points[i].z ) + t;
        pt2_trans /= pt2_trans.at<double>(2,0);
        cout<<"point in the second camera frame: "<<pt2_cam<<endl;
        cout<<"point reprojected from second frame: "<<pt2_trans.t()<<endl;
        cout<<endl;
    }
```

---


  [1]: http://img.blog.csdn.net/20170713114230180?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSGFuc3J5/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast